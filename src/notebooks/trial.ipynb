{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee30615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tree_data_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff2ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 441 rows from CSV...\n",
      "Processed 50/441 rows\n",
      "Processed 100/441 rows\n",
      "Processed 150/441 rows\n",
      "Processed 200/441 rows\n",
      "Processed 250/441 rows\n",
      "Processed 300/441 rows\n",
      "Processed 350/441 rows\n",
      "Processed 400/441 rows\n",
      "\n",
      "✅ Successfully loaded masks for 441/441 rows\n",
      "\n",
      "Sample of loaded masks:\n",
      "  - Pano: zXNDa-3To6LUVVwCusNXAg, View: view_6, Tree: 0-0, Conf: 0.390\n",
      "    Polygon points: 1000\n",
      "  - Pano: zXNDa-3To6LUVVwCusNXAg, View: view_15, Tree: 0-0, Conf: 0.816\n",
      "    Polygon points: 715\n",
      "  - Pano: zn_BsL9aEKPuJ-IlqPplxg, View: view_6, Tree: 0-0, Conf: 0.309\n",
      "    Polygon points: 831\n",
      "  - Pano: zn_BsL9aEKPuJ-IlqPplxg, View: view_15, Tree: 0-0, Conf: 0.779\n",
      "    Polygon points: 708\n",
      "  - Pano: Eg2I2AlO8wExryXAspOx7g, View: view_4, Tree: 0-0, Conf: 0.341\n",
      "    Polygon points: 1044\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Add project root to path to import utilities\n",
    "sys.path.append('/home/utkarsh/TreeInventorization')\n",
    "\n",
    "from src.utils.mask_serialization import load_panorama_masks, deserialize_ultralytics_mask\n",
    "\n",
    "# Define mask directory\n",
    "MASK_DIR = '/home/utkarsh/TreeInventorization/data/masks'\n",
    "\n",
    "def load_mask_for_row(row):\n",
    "    \"\"\"\n",
    "    Load and deserialize mask for a given CSV row.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row containing pano_id and image_path\n",
    "    \n",
    "    Returns:\n",
    "        Deserialized mask data or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract pano_id from the row\n",
    "        pano_id = row['pano_id']\n",
    "        image_path = row['image_path']\n",
    "        \n",
    "        # Construct mask JSON file path\n",
    "        mask_json_path = os.path.join(MASK_DIR, f\"{pano_id}_masks.json\")\n",
    "        \n",
    "        # Check if mask file exists\n",
    "        if not os.path.exists(mask_json_path):\n",
    "            # Silently skip - mask file doesn't exist for this pano\n",
    "            return None\n",
    "        \n",
    "        # Load all masks for this panorama\n",
    "        mask_data = load_panorama_masks(mask_json_path)\n",
    "        \n",
    "        # Extract view number from image path (e.g., \"view15\" from filename)\n",
    "        image_name = os.path.basename(image_path)\n",
    "        view_match = re.search(r'view(\\d+)', image_name)\n",
    "        \n",
    "        if not view_match:\n",
    "            print(f\"⚠️ Could not extract view number from: {image_name}\")\n",
    "            return None\n",
    "            \n",
    "        view_num = view_match.group(1)\n",
    "        view_key = f\"view_{view_num}\"\n",
    "        \n",
    "        # Get masks for this specific view\n",
    "        if view_key in mask_data.get('views', {}):\n",
    "            view_masks = mask_data['views'][view_key]\n",
    "            \n",
    "            # Extract tree and box numbers from filename\n",
    "            tree_box_match = re.search(r'tree(\\d+)_box(\\d+)', image_name)\n",
    "            if tree_box_match:\n",
    "                tree_num = tree_box_match.group(1)\n",
    "                box_num = tree_box_match.group(2)\n",
    "                tree_index = f\"{tree_num}-{box_num}\"\n",
    "                \n",
    "                # Find matching mask by tree_index\n",
    "                for mask in view_masks:\n",
    "                    if mask.get('tree_index') == tree_index:\n",
    "                        # Deserialize the mask\n",
    "                        deserialized = deserialize_ultralytics_mask(mask.get('mask_data', {}))\n",
    "                        \n",
    "                        return {\n",
    "                            'pano_id': pano_id,\n",
    "                            'image_path': image_path,\n",
    "                            'view': view_key,\n",
    "                            'tree_index': tree_index,\n",
    "                            'confidence': mask.get('confidence', 0),\n",
    "                            'mask': deserialized,\n",
    "                            'original_mask_data': mask\n",
    "                        }\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading mask for row: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Process all rows in the dataframe\n",
    "print(f\"Processing {len(df)} rows from CSV...\")\n",
    "mask_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    mask_data = load_mask_for_row(row)\n",
    "    mask_results.append(mask_data)\n",
    "    \n",
    "    # Show progress every 50 rows\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} rows\")\n",
    "\n",
    "# Count successful loads\n",
    "successful_loads = sum(1 for m in mask_results if m is not None)\n",
    "print(f\"\\n✅ Successfully loaded masks for {successful_loads}/{len(df)} rows\")\n",
    "\n",
    "# Show sample of successfully loaded masks\n",
    "if successful_loads > 0:\n",
    "    print(f\"\\nSample of loaded masks:\")\n",
    "    count = 0\n",
    "    for result in mask_results:\n",
    "        if result and count < 5:\n",
    "            print(f\"  - Pano: {result['pano_id']}, View: {result['view']}, Tree: {result['tree_index']}, Conf: {result['confidence']:.3f}\")\n",
    "            if result['mask'] and result['mask'].get('xy'):\n",
    "                print(f\"    Polygon points: {len(result['mask']['xy'][0]) if result['mask']['xy'] else 0}\")\n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15118ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing first mask:\n",
      "  Image: zXNDa-3To6LUVVwCusNXAg_view6_tree0_box0.jpg\n",
      "  Tree index: 0-0\n",
      "  Polygon shape: (1000, 2)\n",
      "  Bounding box: x=[62.0, 981.0], y=[0.0, 449.0]\n",
      "  Box dimensions: 919.0 x 449.0\n",
      "  Binary mask shape: (720, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Additional utility functions for mask processing\n",
    "\n",
    "def get_mask_polygons(mask_data):\n",
    "    \"\"\"Extract polygon coordinates from deserialized mask.\"\"\"\n",
    "    if mask_data and mask_data.get('xy'):\n",
    "        return mask_data['xy'][0]  # Return first polygon\n",
    "    return None\n",
    "\n",
    "def get_mask_binary(mask_data):\n",
    "    \"\"\"Get binary mask if available from deserialized data.\"\"\"\n",
    "    if mask_data and mask_data.get('data') is not None:\n",
    "        return mask_data['data']\n",
    "    return None\n",
    "\n",
    "def get_mask_bbox(polygon):\n",
    "    \"\"\"Calculate bounding box from polygon points.\"\"\"\n",
    "    if polygon is not None and len(polygon) > 0:\n",
    "        x_coords = polygon[:, 0]\n",
    "        y_coords = polygon[:, 1]\n",
    "        return {\n",
    "            'x_min': float(np.min(x_coords)),\n",
    "            'y_min': float(np.min(y_coords)),\n",
    "            'x_max': float(np.max(x_coords)),\n",
    "            'y_max': float(np.max(y_coords)),\n",
    "            'width': float(np.max(x_coords) - np.min(x_coords)),\n",
    "            'height': float(np.max(y_coords) - np.min(y_coords))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Example usage with loaded masks\n",
    "if mask_results:\n",
    "    first_result = next((r for r in mask_results if r), None)\n",
    "    if first_result and first_result.get('mask'):\n",
    "        mask = first_result['mask']\n",
    "        \n",
    "        print(f\"\\nAnalyzing first mask:\")\n",
    "        print(f\"  Image: {os.path.basename(first_result['image_path'])}\")\n",
    "        print(f\"  Tree index: {first_result['tree_index']}\")\n",
    "        \n",
    "        # Get polygon\n",
    "        polygon = get_mask_polygons(mask)\n",
    "        if polygon is not None:\n",
    "            print(f\"  Polygon shape: {polygon.shape}\")\n",
    "            \n",
    "            # Get bounding box\n",
    "            bbox = get_mask_bbox(polygon)\n",
    "            if bbox:\n",
    "                print(f\"  Bounding box: x=[{bbox['x_min']:.1f}, {bbox['x_max']:.1f}], y=[{bbox['y_min']:.1f}, {bbox['y_max']:.1f}]\")\n",
    "                print(f\"  Box dimensions: {bbox['width']:.1f} x {bbox['height']:.1f}\")\n",
    "        \n",
    "        # Check for binary mask\n",
    "        binary_mask = get_mask_binary(mask)\n",
    "        if binary_mask is not None:\n",
    "            print(f\"  Binary mask shape: {binary_mask.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba46190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with masks:\n",
      "  Total rows: 441\n",
      "  Rows with masks: 441\n",
      "  Rows without masks: 0\n",
      "\n",
      "  Average confidence: 0.607\n",
      "  Min confidence: 0.251\n",
      "  Max confidence: 0.986\n",
      "\n",
      "  High confidence masks (>0.7): 172 rows\n"
     ]
    }
   ],
   "source": [
    "# Add mask data to dataframe for easier processing\n",
    "def add_masks_to_dataframe(df, mask_results):\n",
    "    \"\"\"\n",
    "    Add mask data columns to the original dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Original dataframe\n",
    "        mask_results: List of mask results from load_mask_for_row\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional mask columns\n",
    "    \"\"\"\n",
    "    df_with_masks = df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df_with_masks['has_mask'] = False\n",
    "    df_with_masks['mask_confidence'] = np.nan\n",
    "    df_with_masks['mask_polygon'] = None\n",
    "    df_with_masks['mask_bbox'] = None\n",
    "    df_with_masks['mask_data'] = None\n",
    "    \n",
    "    # Add mask data to corresponding rows\n",
    "    for idx, mask_result in enumerate(mask_results):\n",
    "        if mask_result is not None:\n",
    "            df_with_masks.at[idx, 'has_mask'] = True\n",
    "            df_with_masks.at[idx, 'mask_confidence'] = mask_result.get('confidence', 0)\n",
    "            \n",
    "            # Get polygon if available\n",
    "            if mask_result.get('mask') and mask_result['mask'].get('xy'):\n",
    "                polygon = mask_result['mask']['xy'][0]\n",
    "                df_with_masks.at[idx, 'mask_polygon'] = polygon\n",
    "                \n",
    "                # Calculate and store bbox\n",
    "                bbox = get_mask_bbox(polygon)\n",
    "                df_with_masks.at[idx, 'mask_bbox'] = bbox\n",
    "            \n",
    "            # Store full mask data\n",
    "            df_with_masks.at[idx, 'mask_data'] = mask_result\n",
    "    \n",
    "    return df_with_masks\n",
    "\n",
    "# Create enhanced dataframe with mask data\n",
    "df_with_masks = add_masks_to_dataframe(df, mask_results)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nDataFrame with masks:\")\n",
    "print(f\"  Total rows: {len(df_with_masks)}\")\n",
    "print(f\"  Rows with masks: {df_with_masks['has_mask'].sum()}\")\n",
    "print(f\"  Rows without masks: {(~df_with_masks['has_mask']).sum()}\")\n",
    "\n",
    "if df_with_masks['has_mask'].any():\n",
    "    print(f\"\\n  Average confidence: {df_with_masks['mask_confidence'].mean():.3f}\")\n",
    "    print(f\"  Min confidence: {df_with_masks['mask_confidence'].min():.3f}\")\n",
    "    print(f\"  Max confidence: {df_with_masks['mask_confidence'].max():.3f}\")\n",
    "\n",
    "# Example: Filter rows with high confidence masks\n",
    "high_conf_masks = df_with_masks[df_with_masks['mask_confidence'] > 0.7]\n",
    "print(f\"\\n  High confidence masks (>0.7): {len(high_conf_masks)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "723d6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef0831b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import this module and call run_experiment(...). See docstring for usage.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mask Usability Classifier — v2 (binary: usable vs not)\n",
    "------------------------------------------------------\n",
    "Changes in this version:\n",
    "  • Keeps square input (default 224) and adds correct ImageNet normalization.\n",
    "  • Joint image↔mask horizontal flip for Fusion so they stay aligned.\n",
    "  • Backbone selector for the image branch: resnet18, resnet50, efficientnet_b0.\n",
    "  • Always returns tensors from Dataset (no None), so default collate works.\n",
    "  • Saves backbone + normalization in checkpoints and uses them at inference.\n",
    "\n",
    "Variants supported:\n",
    "  1) mask_cnn         — Tiny CNN on the mask only (kept for completeness)\n",
    "  2) resnet_overlay   — Transfer learning on overlayed images (recommended)\n",
    "  3) fusion           — Image + Mask late-fusion\n",
    "\n",
    "Quick start:\n",
    "    results = run_experiment(\n",
    "        df_with_masks,\n",
    "        variant=\"resnet_overlay\",            # or \"fusion\"\n",
    "        backbone=\"resnet50\",                 # \"resnet18\" | \"resnet50\" | \"efficientnet_b0\"\n",
    "        out_dir=\"/mnt/data/mask_quality/res50\",\n",
    "        image_size=224,\n",
    "        num_epochs=30,\n",
    "        batch_size=16,\n",
    "    )\n",
    "\n",
    "    infer = load_for_inference(\"/mnt/data/mask_quality/res50/best_overall.pt\")\n",
    "    is_ok, prob = predict_row(df_with_masks.iloc[0], infer)\n",
    "    print(is_ok, prob)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    from torchvision import transforms\n",
    "    from torchvision.models import (\n",
    "        resnet18, ResNet18_Weights,\n",
    "        resnet50, ResNet50_Weights,\n",
    "        efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"torchvision is required. pip install torchvision\")\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def _safe_makedirs(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "def _to_bool_label(x: Any) -> int:\n",
    "    \"\"\"Map your 'correct' column values to 1/0. Accepts yes/no, 1/0, True/False.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"yes\", \"true\", \"1\"}:  # usable\n",
    "            return 1\n",
    "        if s in {\"no\", \"false\", \"0\"}:  # not usable\n",
    "            return 0\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return int(x)\n",
    "    if isinstance(x, bool):\n",
    "        return int(x)\n",
    "    raise ValueError(f\"Unrecognized label value: {x}\")\n",
    "\n",
    "\n",
    "def extract_mask_array(mask_obj: Any, orig_shape: Optional[Tuple[int, int]] = None) -> np.ndarray:\n",
    "    \"\"\"Return a 2D uint8 mask {0,1} from various formats (np, dict->data, RLE, polygon).\"\"\"\n",
    "    if isinstance(mask_obj, np.ndarray):\n",
    "        m = mask_obj\n",
    "        if m.ndim == 3 and m.shape[-1] == 1:\n",
    "            m = m[..., 0]\n",
    "        return (m > 0.5).astype(np.uint8)\n",
    "\n",
    "    if not isinstance(mask_obj, dict):\n",
    "        raise ValueError(\"mask_obj must be np.ndarray or dict\")\n",
    "\n",
    "    def _get(key):\n",
    "        return mask_obj.get(key) if isinstance(mask_obj, dict) else None\n",
    "\n",
    "    # Try nested dicts with direct 'data'\n",
    "    for k in [\"mask\", \"mask_data\", \"data\", \"rle\"]:\n",
    "        d = _get(k)\n",
    "        if isinstance(d, dict) and \"data\" in d and isinstance(d[\"data\"], np.ndarray):\n",
    "            m = d[\"data\"]\n",
    "            if m.ndim == 3 and m.shape[-1] == 1:\n",
    "                m = m[..., 0]\n",
    "            return (m > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Try top-level 'data'\n",
    "    if \"data\" in mask_obj and isinstance(mask_obj[\"data\"], np.ndarray):\n",
    "        m = mask_obj[\"data\"]\n",
    "        if m.ndim == 3 and m.shape[-1] == 1:\n",
    "            m = m[..., 0]\n",
    "        return (m > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Try RLE\n",
    "    rle = None\n",
    "    for k in [\"mask\", \"mask_data\", \"rle\"]:\n",
    "        d = _get(k)\n",
    "        if isinstance(d, dict) and \"rle\" in d:\n",
    "            rle = d[\"rle\"]\n",
    "            break\n",
    "    if rle is not None and isinstance(rle, dict) and \"counts\" in rle and \"size\" in rle:\n",
    "        try:\n",
    "            from pycocotools import mask as maskUtils  # type: ignore\n",
    "            rr = {\n",
    "                \"size\": rle[\"size\"],\n",
    "                \"counts\": rle[\"counts\"] if isinstance(rle[\"counts\"], (bytes, bytearray)) else rle[\"counts\"].encode(\"ascii\"),\n",
    "            }\n",
    "            m = maskUtils.decode(rr)\n",
    "            if m.ndim == 3:\n",
    "                m = m[:, :, 0]\n",
    "            return (m > 0).astype(np.uint8)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Try polygons\n",
    "    polys = None\n",
    "    for k in [\"mask\", \"mask_data\"]:\n",
    "        d = _get(k)\n",
    "        if isinstance(d, dict) and \"xy\" in d:\n",
    "            polys = d[\"xy\"]\n",
    "            break\n",
    "    if polys is not None:\n",
    "        if isinstance(polys, np.ndarray):\n",
    "            polys = [polys]\n",
    "        if orig_shape is None:\n",
    "            for k in [\"mask\", \"mask_data\"]:\n",
    "                d = _get(k)\n",
    "                if isinstance(d, dict) and \"orig_shape\" in d:\n",
    "                    osz = d[\"orig_shape\"]\n",
    "                    if isinstance(osz, (list, tuple)) and len(osz) >= 2:\n",
    "                        orig_shape = (int(osz[0]), int(osz[1]))\n",
    "                        break\n",
    "        if orig_shape is None:\n",
    "            xs, ys = [], []\n",
    "            for p in polys:\n",
    "                xs.append(np.max(p[:, 0])); ys.append(np.max(p[:, 1]))\n",
    "            H = int(max(ys) + 1); W = int(max(xs) + 1)\n",
    "            orig_shape = (H, W)\n",
    "        H, W = orig_shape\n",
    "        m = np.zeros((H, W), dtype=np.uint8)\n",
    "        try:\n",
    "            import cv2\n",
    "            for p in polys:\n",
    "                cv2.fillPoly(m, [p.astype(np.int32)], 1)\n",
    "            return m\n",
    "        except Exception:\n",
    "            for p in polys:\n",
    "                p = p.astype(int)\n",
    "                for y in range(p[:, 1].min(), p[:, 1].max() + 1):\n",
    "                    xs = p[p[:, 1] == y][:, 0]\n",
    "                    if len(xs) >= 2:\n",
    "                        m[y, xs.min(): xs.max()+1] = 1\n",
    "            return m\n",
    "\n",
    "    raise ValueError(\"Could not extract a binary mask from provided mask_obj\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Backbones for the image branch\n",
    "# -----------------------------\n",
    "\n",
    "def make_image_backbone(name: str = \"resnet18\", pretrained: bool = True):\n",
    "    name = name.lower()\n",
    "    if name == \"resnet18\":\n",
    "        weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        m = resnet18(weights=weights)\n",
    "        in_feats = m.fc.in_features\n",
    "        m.fc = nn.Identity()\n",
    "        norm = None\n",
    "        if weights is not None:\n",
    "            t = weights.transforms()\n",
    "            norm = {\"mean\": list(t.mean), \"std\": list(t.std)}\n",
    "        return m, in_feats, norm\n",
    "    if name == \"resnet50\":\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "        m = resnet50(weights=weights)\n",
    "        in_feats = m.fc.in_features\n",
    "        m.fc = nn.Identity()\n",
    "        norm = None\n",
    "        if weights is not None:\n",
    "            t = weights.transforms()\n",
    "            norm = {\"mean\": list(t.mean), \"std\": list(t.std)}\n",
    "        return m, in_feats, norm\n",
    "    if name == \"efficientnet_b0\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        m = efficientnet_b0(weights=weights)\n",
    "        in_feats = m.classifier[1].in_features\n",
    "        m.classifier = nn.Identity()\n",
    "        norm = None\n",
    "        if weights is not None:\n",
    "            t = weights.transforms()\n",
    "            norm = {\"mean\": list(t.mean), \"std\": list(t.std)}\n",
    "        return m, in_feats, norm\n",
    "    raise ValueError(\"unknown backbone; use resnet18 | resnet50 | efficientnet_b0\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class MaskQualityDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 use_overlay: bool = True,\n",
    "                 use_mask: bool = True,\n",
    "                 image_key: str = \"image_path\",\n",
    "                 mask_key: str = \"mask_data\",\n",
    "                 label_key: str = \"correct\",\n",
    "                 image_size: int = 224,\n",
    "                 image_norm: Optional[Dict[str, List[float]]] = None,\n",
    "                 augment: bool = True,\n",
    "                 joint_flip_for_fusion: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_overlay = use_overlay\n",
    "        self.use_mask = use_mask\n",
    "        self.image_key = image_key\n",
    "        self.mask_key = mask_key\n",
    "        self.label_key = label_key\n",
    "        self.image_size = int(image_size)\n",
    "        self.image_norm = image_norm\n",
    "        self.augment = augment\n",
    "        self.joint_flip_for_fusion = joint_flip_for_fusion\n",
    "\n",
    "        # Pre-build simple transforms that do NOT include random flip (we'll do it jointly)\n",
    "        img_tf_list = [transforms.Resize((self.image_size, self.image_size))]\n",
    "        if augment:\n",
    "            img_tf_list.append(transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)], p=0.3))\n",
    "        img_tf_list.append(transforms.ToTensor())\n",
    "        if image_norm is not None:\n",
    "            img_tf_list.append(transforms.Normalize(mean=image_norm[\"mean\"], std=image_norm[\"std\"]))\n",
    "        self.img_tf = transforms.Compose(img_tf_list) if use_overlay else None\n",
    "\n",
    "        self.mask_tf = None\n",
    "        if use_mask:\n",
    "            self.mask_tf = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "            ])\n",
    "\n",
    "        self.labels = np.array([_to_bool_label(x) for x in self.df[self.label_key].values], dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        # Prepare placeholders so collate never sees None\n",
    "        img_t = torch.zeros(3, self.image_size, self.image_size)\n",
    "        mask_t = torch.zeros(1, self.image_size, self.image_size)\n",
    "\n",
    "        # Load image/mask\n",
    "        if self.use_overlay:\n",
    "            try:\n",
    "                with Image.open(row[self.image_key]) as im:\n",
    "                    im = im.convert(\"RGB\")\n",
    "                    img_pil = im\n",
    "            except Exception:\n",
    "                img_pil = Image.new(\"RGB\", (self.image_size, self.image_size))\n",
    "        if self.use_mask:\n",
    "            try:\n",
    "                m = extract_mask_array(row[self.mask_key])\n",
    "            except Exception:\n",
    "                m = np.zeros((self.image_size, self.image_size), dtype=np.uint8)\n",
    "\n",
    "        # Joint flip if both branches are used\n",
    "        if self.augment and self.joint_flip_for_fusion and self.use_overlay and self.use_mask:\n",
    "            if np.random.rand() < 0.5:\n",
    "                img_pil = transforms.functional.hflip(img_pil)\n",
    "                m = np.fliplr(m).copy()\n",
    "\n",
    "        # Apply per-branch transforms\n",
    "        if self.use_overlay:\n",
    "            img_t = self.img_tf(img_pil)\n",
    "        if self.use_mask:\n",
    "            m = (m > 0).astype(np.uint8)\n",
    "            mask_t = self.mask_tf(m)\n",
    "\n",
    "        return img_t, mask_t, y\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "class TinyMaskCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(), nn.Dropout(0.3), nn.Linear(128, 1))\n",
    "    def forward(self, x_mask: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.net(x_mask)\n",
    "        return self.head(h).squeeze(1)  # logits\n",
    "\n",
    "\n",
    "class ImageOverlayClassifier(nn.Module):\n",
    "    def __init__(self, backbone: str = \"resnet18\", pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.backbone, in_feats, self.norm = make_image_backbone(backbone, pretrained)\n",
    "        self.head = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_feats, 1))\n",
    "        self.backbone_name = backbone\n",
    "    def forward(self, x_img: torch.Tensor) -> torch.Tensor:\n",
    "        f = self.backbone(x_img)\n",
    "        return self.head(f).squeeze(1)\n",
    "\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    def __init__(self, backbone: str = \"resnet18\", pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.img_backbone, in_feats, self.norm = make_image_backbone(backbone, pretrained)\n",
    "        self.mask_branch = TinyMaskCNN()\n",
    "        self.head = nn.Sequential(nn.Dropout(0.3), nn.Linear(in_feats + 128, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256, 1))\n",
    "        self.backbone_name = backbone\n",
    "    def forward(self, x_img: torch.Tensor, x_mask: torch.Tensor) -> torch.Tensor:\n",
    "        fi = self.img_backbone(x_img)\n",
    "        fm = self.mask_branch.net(x_mask)\n",
    "        fm = self.mask_branch.head[0:2](fm)  # Flatten + Dropout\n",
    "        fm = fm.view(fm.size(0), -1)\n",
    "        return self.head(torch.cat([fi, fm], dim=1)).squeeze(1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training / Eval\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    variant: str = \"resnet_overlay\"  # 'mask_cnn' | 'resnet_overlay' | 'fusion'\n",
    "    backbone: str = \"resnet18\"       # image branch backbone\n",
    "    out_dir: str = \"/mnt/data/mask_quality\"\n",
    "    num_epochs: int = 30\n",
    "    batch_size: int = 16\n",
    "    image_size: int = 224\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    early_stop_patience: int = 5\n",
    "    num_folds: int = 5\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "def _make_model(variant: str, backbone: str) -> nn.Module:\n",
    "    if variant == \"mask_cnn\":\n",
    "        return TinyMaskCNN()\n",
    "    elif variant == \"resnet_overlay\":\n",
    "        return ImageOverlayClassifier(backbone=backbone, pretrained=True)\n",
    "    elif variant == \"fusion\":\n",
    "        return FusionNet(backbone=backbone, pretrained=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown variant\")\n",
    "\n",
    "\n",
    "def _get_device() -> torch.device:\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def _compute_class_weight(y: np.ndarray) -> torch.Tensor:\n",
    "    pos = (y == 1).sum(); neg = (y == 0).sum()\n",
    "    if pos == 0:\n",
    "        return torch.tensor(1.0)\n",
    "    return torch.tensor(max(1.0, neg / max(1, pos)), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def _metrics_from_logits(logits: torch.Tensor, y_true: torch.Tensor, threshold: float = 0.5) -> Dict[str, Any]:\n",
    "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "    y = y_true.detach().cpu().numpy()\n",
    "    yhat = (probs >= threshold).astype(int)\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "    acc = accuracy_score(y, yhat)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y, yhat, average='binary', zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y, probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    cm = confusion_matrix(y, yhat).tolist()\n",
    "    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc, \"confusion_matrix\": cm, \"threshold\": threshold}\n",
    "\n",
    "\n",
    "def _optimal_threshold(logits: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    from sklearn.metrics import roc_curve\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    fpr, tpr, thr = roc_curve(y_true, probs)\n",
    "    j = tpr - fpr\n",
    "    i = int(np.argmax(j))\n",
    "    return float(thr[i])\n",
    "\n",
    "\n",
    "def _train_one_epoch(model, loader, device, optimizer, criterion):\n",
    "    model.train(); total_loss = 0.0\n",
    "    for img_t, mask_t, y in loader:\n",
    "        y = y.float().to(device)\n",
    "        if isinstance(model, FusionNet):\n",
    "            img_t = img_t.to(device); mask_t = mask_t.to(device)\n",
    "            logits = model(img_t, mask_t)\n",
    "        elif isinstance(model, ImageOverlayClassifier):\n",
    "            img_t = img_t.to(device)\n",
    "            logits = model(img_t)\n",
    "        else:\n",
    "            mask_t = mask_t.to(device)\n",
    "            logits = model(mask_t)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad(set_to_none=True); loss.backward(); optimizer.step()\n",
    "        total_loss += float(loss.item()) * y.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def _eval_logits(model, loader, device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval(); ys, lg = [], []\n",
    "    with torch.no_grad():\n",
    "        for img_t, mask_t, y in loader:\n",
    "            y = y.to(device)\n",
    "            if isinstance(model, FusionNet):\n",
    "                img_t = img_t.to(device); mask_t = mask_t.to(device)\n",
    "                logits = model(img_t, mask_t)\n",
    "            elif isinstance(model, ImageOverlayClassifier):\n",
    "                img_t = img_t.to(device)\n",
    "                logits = model(img_t)\n",
    "            else:\n",
    "                mask_t = mask_t.to(device)\n",
    "                logits = model(mask_t)\n",
    "            ys.append(y.cpu().numpy()); lg.append(logits.cpu().numpy())\n",
    "    return np.concatenate(lg), np.concatenate(ys)\n",
    "\n",
    "\n",
    "def run_experiment(df: pd.DataFrame,\n",
    "                   variant: str = \"resnet_overlay\",\n",
    "                   backbone: str = \"resnet18\",\n",
    "                   out_dir: str = \"/mnt/data/mask_quality\",\n",
    "                   num_epochs: int = 30,\n",
    "                   batch_size: int = 16,\n",
    "                   image_size: int = 224,\n",
    "                   seed: int = 42,\n",
    "                   num_folds: int = 5,\n",
    "                   early_stop_patience: int = 5,\n",
    "                   lr: float = 1e-4,\n",
    "                   weight_decay: float = 1e-4) -> Dict[str, Any]:\n",
    "    \"\"\"Train + evaluate with K-fold CV. Returns metrics summary and saves best checkpoint.\"\"\"\n",
    "    _safe_makedirs(out_dir)\n",
    "\n",
    "    # ensure labels are available\n",
    "    y_all = np.array([_to_bool_label(x) for x in df['correct'].values], dtype=np.int64)\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    device = _get_device()\n",
    "    fold_summaries: List[Dict[str, Any]] = []\n",
    "    best_overall = {\"f1\": -1.0, \"path\": None}\n",
    "\n",
    "    # Build a temp backbone to get normalization stats\n",
    "    tmp_backbone, _, norm = make_image_backbone(backbone, pretrained=True)\n",
    "    del tmp_backbone\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros_like(y_all), y_all), start=1):\n",
    "        df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "        df_va = df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "        use_overlay = (variant != 'mask_cnn')\n",
    "        use_mask = (variant != 'resnet_overlay')\n",
    "\n",
    "        ds_tr = MaskQualityDataset(df_tr,\n",
    "                                   use_overlay=use_overlay,\n",
    "                                   use_mask=use_mask,\n",
    "                                   image_size=image_size,\n",
    "                                   image_norm=norm,\n",
    "                                   augment=True,\n",
    "                                   joint_flip_for_fusion=True)\n",
    "        ds_va = MaskQualityDataset(df_va,\n",
    "                                   use_overlay=use_overlay,\n",
    "                                   use_mask=use_mask,\n",
    "                                   image_size=image_size,\n",
    "                                   image_norm=norm,\n",
    "                                   augment=False,\n",
    "                                   joint_flip_for_fusion=False)\n",
    "        dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        model = _make_model(variant, backbone).to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        pos_weight = _compute_class_weight(np.array([_to_bool_label(x) for x in df_tr['correct'].values])).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        best_state = None; best_f1 = -1.0; best_thr = 0.5; no_improve = 0\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            tr_loss = _train_one_epoch(model, dl_tr, device, optimizer, criterion)\n",
    "            logits_va, y_va = _eval_logits(model, dl_va, device)\n",
    "            thr = _optimal_threshold(logits_va, y_va)\n",
    "            m = _metrics_from_logits(torch.tensor(logits_va), torch.tensor(y_va), threshold=thr)\n",
    "            print(f\"Fold {fold:02d} | Epoch {epoch:02d} | loss {tr_loss:.4f} | val F1 {m['f1']:.3f} | thr {thr:.3f}\")\n",
    "            if m['f1'] > best_f1 + 1e-4:\n",
    "                best_f1 = m['f1']; best_thr = thr\n",
    "                best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= early_stop_patience:\n",
    "                    break\n",
    "\n",
    "        assert best_state is not None, \"Training did not improve at all. Check data.\"\n",
    "        model.load_state_dict(best_state)\n",
    "        logits_va, y_va = _eval_logits(model, dl_va, device)\n",
    "        fold_metrics = _metrics_from_logits(torch.tensor(logits_va), torch.tensor(y_va), threshold=best_thr)\n",
    "\n",
    "        ckpt_path = os.path.join(out_dir, f\"{variant}_{backbone}_fold{fold}.pt\")\n",
    "        torch.save({\n",
    "            \"variant\": variant,\n",
    "            \"backbone\": backbone,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"threshold\": best_thr,\n",
    "            \"image_size\": image_size,\n",
    "            \"norm_mean\": (norm[\"mean\"] if norm else None),\n",
    "            \"norm_std\": (norm[\"std\"] if norm else None),\n",
    "        }, ckpt_path)\n",
    "        fold_metrics.update({\"ckpt_path\": ckpt_path, \"fold\": fold})\n",
    "        fold_summaries.append(fold_metrics)\n",
    "\n",
    "        if fold_metrics['f1'] > best_overall['f1']:\n",
    "            best_overall = {\"f1\": fold_metrics['f1'], \"path\": ckpt_path}\n",
    "\n",
    "    def _agg(key):\n",
    "        vals = [f[key] for f in fold_summaries if isinstance(f.get(key), (int, float)) and not math.isnan(f[key])]\n",
    "        return float(np.mean(vals)), float(np.std(vals))\n",
    "\n",
    "    acc_m, acc_s = _agg('acc'); prec_m, prec_s = _agg('prec'); rec_m, rec_s = _agg('rec')\n",
    "    f1_m, f1_s = _agg('f1'); auc_m, auc_s = _agg('auc')\n",
    "\n",
    "    model_card = {\n",
    "        \"variant\": variant,\n",
    "        \"backbone\": backbone,\n",
    "        \"folds\": fold_summaries,\n",
    "        \"summary\": {\n",
    "            \"acc_mean\": acc_m, \"acc_std\": acc_s,\n",
    "            \"prec_mean\": prec_m, \"prec_std\": prec_s,\n",
    "            \"rec_mean\": rec_m, \"rec_std\": rec_s,\n",
    "            \"f1_mean\": f1_m, \"f1_std\": f1_s,\n",
    "            \"auc_mean\": auc_m, \"auc_std\": auc_s,\n",
    "        },\n",
    "        \"best_overall\": best_overall,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(out_dir, \"model_card.json\"), \"w\") as f:\n",
    "        json.dump(model_card, f, indent=2)\n",
    "\n",
    "    if best_overall['path'] is not None:\n",
    "        best_dst = os.path.join(out_dir, \"best_overall.pt\")\n",
    "        if os.path.abspath(best_dst) != os.path.abspath(best_overall['path']):\n",
    "            import shutil; shutil.copy2(best_overall['path'], best_dst)\n",
    "        model_card[\"best_overall\"][\"path\"] = best_dst\n",
    "\n",
    "    return model_card\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Inference helpers\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class InferenceBundle:\n",
    "    variant: str\n",
    "    backbone: str\n",
    "    model: nn.Module\n",
    "    threshold: float\n",
    "    image_size: int\n",
    "    device: torch.device\n",
    "    norm_mean: Optional[List[float]]\n",
    "    norm_std: Optional[List[float]]\n",
    "\n",
    "\n",
    "def load_for_inference(ckpt_path: str) -> InferenceBundle:\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    variant = ckpt[\"variant\"]\n",
    "    backbone = ckpt.get(\"backbone\", \"resnet18\")\n",
    "    model = _make_model(variant, backbone)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    device = _get_device(); model.to(device)\n",
    "    thr = float(ckpt.get(\"threshold\", 0.5))\n",
    "    image_size = int(ckpt.get(\"image_size\", 224))\n",
    "    norm_mean = ckpt.get(\"norm_mean\", None)\n",
    "    norm_std = ckpt.get(\"norm_std\", None)\n",
    "    return InferenceBundle(variant, backbone, model, thr, image_size, device, norm_mean, norm_std)\n",
    "\n",
    "\n",
    "def _build_image_infer_tf(image_size: int, norm_mean: Optional[List[float]], norm_std: Optional[List[float]]):\n",
    "    tfs = [transforms.Resize((image_size, image_size)), transforms.ToTensor()]\n",
    "    if norm_mean is not None and norm_std is not None:\n",
    "        tfs.append(transforms.Normalize(mean=norm_mean, std=norm_std))\n",
    "    return transforms.Compose(tfs)\n",
    "\n",
    "\n",
    "def predict_row(row: pd.Series, infer: InferenceBundle,\n",
    "                image_key: str = \"image_path\",\n",
    "                mask_key: str = \"mask_data\") -> Tuple[bool, float]:\n",
    "    \"\"\"Return (bool_is_usable, probability_of_usable).\"\"\"\n",
    "    img_t = torch.zeros(1, 3, infer.image_size, infer.image_size, device=infer.device)\n",
    "    mask_t = torch.zeros(1, 1, infer.image_size, infer.image_size, device=infer.device)\n",
    "\n",
    "    if infer.variant != 'mask_cnn':\n",
    "        with Image.open(row[image_key]) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            tf = _build_image_infer_tf(infer.image_size, infer.norm_mean, infer.norm_std)\n",
    "            img_t = tf(im).unsqueeze(0).to(infer.device)\n",
    "\n",
    "    if infer.variant != 'resnet_overlay':\n",
    "        m = extract_mask_array(row[mask_key])\n",
    "        m = (m > 0).astype(np.uint8)\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Resize((infer.image_size, infer.image_size))])\n",
    "        mask_t = tfm(m).unsqueeze(0).to(infer.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if infer.variant == 'fusion':\n",
    "            logits = infer.model(img_t, mask_t)\n",
    "        elif infer.variant == 'resnet_overlay':\n",
    "            logits = infer.model(img_t)\n",
    "        else:\n",
    "            logits = infer.model(mask_t)\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        return (prob >= infer.threshold), float(prob)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Import this module and call run_experiment(...). See docstring for usage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "102eba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate to allow None for image or mask depending on variant\n",
    "import torch\n",
    "\n",
    "def collate_mask_quality(batch):\n",
    "    imgs, masks, ys = zip(*batch)\n",
    "    # y as tensor\n",
    "    y_batch = torch.tensor(ys, dtype=torch.int64)\n",
    "\n",
    "    # stack only when present; otherwise keep None\n",
    "    img_batch = None\n",
    "    mask_batch = None\n",
    "    if any(t is not None for t in imgs):\n",
    "        img_batch = torch.stack([t for t in imgs if t is not None])\n",
    "    if any(t is not None for t in masks):\n",
    "        mask_batch = torch.stack([t for t in masks if t is not None])\n",
    "\n",
    "    return img_batch, mask_batch, y_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dcb896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | Epoch 01 | loss 0.7405 | val F1 0.789 | thr 0.520\n",
      "Fold 01 | Epoch 02 | loss 0.2454 | val F1 0.800 | thr 0.497\n",
      "Fold 01 | Epoch 03 | loss 0.0814 | val F1 0.783 | thr 0.504\n",
      "Fold 01 | Epoch 04 | loss 0.0534 | val F1 0.800 | thr 0.322\n",
      "Fold 01 | Epoch 05 | loss 0.0451 | val F1 0.806 | thr 0.289\n",
      "Fold 01 | Epoch 06 | loss 0.0463 | val F1 0.795 | thr 0.154\n",
      "Fold 01 | Epoch 07 | loss 0.0410 | val F1 0.775 | thr 0.192\n",
      "Fold 01 | Epoch 08 | loss 0.0297 | val F1 0.757 | thr 0.048\n",
      "Fold 01 | Epoch 09 | loss 0.0204 | val F1 0.800 | thr 0.271\n",
      "Fold 01 | Epoch 10 | loss 0.0169 | val F1 0.818 | thr 0.503\n",
      "Fold 01 | Epoch 11 | loss 0.0096 | val F1 0.824 | thr 0.477\n",
      "Fold 01 | Epoch 12 | loss 0.0150 | val F1 0.800 | thr 0.391\n",
      "Fold 01 | Epoch 13 | loss 0.0117 | val F1 0.818 | thr 0.371\n",
      "Fold 01 | Epoch 14 | loss 0.0083 | val F1 0.818 | thr 0.392\n",
      "Fold 01 | Epoch 15 | loss 0.0038 | val F1 0.831 | thr 0.446\n",
      "Fold 01 | Epoch 16 | loss 0.0118 | val F1 0.794 | thr 0.312\n",
      "Fold 01 | Epoch 17 | loss 0.0037 | val F1 0.794 | thr 0.257\n",
      "Fold 01 | Epoch 18 | loss 0.0198 | val F1 0.784 | thr 0.116\n",
      "Fold 01 | Epoch 19 | loss 0.0136 | val F1 0.783 | thr 0.087\n",
      "Fold 01 | Epoch 20 | loss 0.0075 | val F1 0.824 | thr 0.162\n",
      "Fold 02 | Epoch 01 | loss 0.7321 | val F1 0.839 | thr 0.818\n",
      "Fold 02 | Epoch 02 | loss 0.2029 | val F1 0.812 | thr 0.544\n",
      "Fold 02 | Epoch 03 | loss 0.0609 | val F1 0.842 | thr 0.796\n",
      "Fold 02 | Epoch 04 | loss 0.0327 | val F1 0.839 | thr 0.565\n",
      "Fold 02 | Epoch 05 | loss 0.0210 | val F1 0.833 | thr 0.293\n",
      "Fold 02 | Epoch 06 | loss 0.0167 | val F1 0.800 | thr 0.156\n",
      "Fold 02 | Epoch 07 | loss 0.0193 | val F1 0.814 | thr 0.257\n",
      "Fold 02 | Epoch 08 | loss 0.0154 | val F1 0.794 | thr 0.095\n",
      "Fold 03 | Epoch 01 | loss 0.7788 | val F1 0.714 | thr 0.885\n",
      "Fold 03 | Epoch 02 | loss 0.2393 | val F1 0.757 | thr 0.514\n",
      "Fold 03 | Epoch 03 | loss 0.0884 | val F1 0.716 | thr 0.709\n",
      "Fold 03 | Epoch 04 | loss 0.0616 | val F1 0.767 | thr 0.662\n",
      "Fold 03 | Epoch 05 | loss 0.0325 | val F1 0.727 | thr 0.209\n",
      "Fold 03 | Epoch 06 | loss 0.0320 | val F1 0.761 | thr 0.097\n",
      "Fold 03 | Epoch 07 | loss 0.0143 | val F1 0.794 | thr 0.417\n",
      "Fold 03 | Epoch 08 | loss 0.0373 | val F1 0.769 | thr 0.096\n",
      "Fold 03 | Epoch 09 | loss 0.0207 | val F1 0.780 | thr 0.207\n",
      "Fold 03 | Epoch 10 | loss 0.0150 | val F1 0.800 | thr 0.297\n",
      "Fold 03 | Epoch 11 | loss 0.0170 | val F1 0.714 | thr 0.323\n",
      "Fold 03 | Epoch 12 | loss 0.0513 | val F1 0.759 | thr 0.766\n",
      "Fold 03 | Epoch 13 | loss 0.0759 | val F1 0.776 | thr 0.132\n",
      "Fold 03 | Epoch 14 | loss 0.0811 | val F1 0.722 | thr 0.064\n",
      "Fold 03 | Epoch 15 | loss 0.0392 | val F1 0.783 | thr 0.267\n",
      "Fold 04 | Epoch 01 | loss 0.7221 | val F1 0.818 | thr 0.928\n",
      "Fold 04 | Epoch 02 | loss 0.2351 | val F1 0.806 | thr 0.873\n",
      "Fold 04 | Epoch 03 | loss 0.0752 | val F1 0.818 | thr 0.703\n",
      "Fold 04 | Epoch 04 | loss 0.0384 | val F1 0.844 | thr 0.454\n",
      "Fold 04 | Epoch 05 | loss 0.0174 | val F1 0.852 | thr 0.609\n",
      "Fold 04 | Epoch 06 | loss 0.0158 | val F1 0.852 | thr 0.434\n",
      "Fold 04 | Epoch 07 | loss 0.0217 | val F1 0.825 | thr 0.392\n",
      "Fold 04 | Epoch 08 | loss 0.0180 | val F1 0.812 | thr 0.468\n",
      "Fold 04 | Epoch 09 | loss 0.0139 | val F1 0.839 | thr 0.963\n",
      "Fold 04 | Epoch 10 | loss 0.0113 | val F1 0.839 | thr 0.759\n",
      "Fold 05 | Epoch 01 | loss 0.7225 | val F1 0.829 | thr 0.691\n",
      "Fold 05 | Epoch 02 | loss 0.2269 | val F1 0.881 | thr 0.547\n",
      "Fold 05 | Epoch 03 | loss 0.0630 | val F1 0.852 | thr 0.358\n",
      "Fold 05 | Epoch 04 | loss 0.0382 | val F1 0.866 | thr 0.111\n",
      "Fold 05 | Epoch 05 | loss 0.0284 | val F1 0.853 | thr 0.176\n",
      "Fold 05 | Epoch 06 | loss 0.0163 | val F1 0.873 | thr 0.849\n",
      "Fold 05 | Epoch 07 | loss 0.0136 | val F1 0.871 | thr 0.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/utkarsh/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:08<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | Epoch 01 | loss 0.8826 | val F1 0.758 | thr 0.497\n",
      "Fold 01 | Epoch 02 | loss 0.6270 | val F1 0.776 | thr 0.556\n",
      "Fold 01 | Epoch 03 | loss 0.2931 | val F1 0.789 | thr 0.259\n",
      "Fold 01 | Epoch 04 | loss 0.1258 | val F1 0.853 | thr 0.447\n",
      "Fold 01 | Epoch 05 | loss 0.0642 | val F1 0.781 | thr 0.288\n",
      "Fold 01 | Epoch 06 | loss 0.0612 | val F1 0.778 | thr 0.067\n",
      "Fold 01 | Epoch 07 | loss 0.0262 | val F1 0.812 | thr 0.111\n",
      "Fold 01 | Epoch 08 | loss 0.0135 | val F1 0.831 | thr 0.605\n",
      "Fold 01 | Epoch 09 | loss 0.0100 | val F1 0.825 | thr 0.478\n",
      "Fold 02 | Epoch 01 | loss 0.8650 | val F1 0.759 | thr 0.564\n",
      "Fold 02 | Epoch 02 | loss 0.5784 | val F1 0.806 | thr 0.720\n",
      "Fold 02 | Epoch 03 | loss 0.3026 | val F1 0.781 | thr 0.313\n",
      "Fold 02 | Epoch 04 | loss 0.1297 | val F1 0.828 | thr 0.819\n",
      "Fold 02 | Epoch 05 | loss 0.0568 | val F1 0.800 | thr 0.531\n",
      "Fold 02 | Epoch 06 | loss 0.0321 | val F1 0.842 | thr 0.729\n",
      "Fold 02 | Epoch 07 | loss 0.0292 | val F1 0.807 | thr 0.705\n",
      "Fold 02 | Epoch 08 | loss 0.0193 | val F1 0.800 | thr 0.706\n",
      "Fold 02 | Epoch 09 | loss 0.0149 | val F1 0.767 | thr 0.578\n",
      "Fold 02 | Epoch 10 | loss 0.0493 | val F1 0.754 | thr 0.310\n",
      "Fold 02 | Epoch 11 | loss 0.0577 | val F1 0.743 | thr 0.097\n",
      "Fold 03 | Epoch 01 | loss 0.8599 | val F1 0.757 | thr 0.538\n",
      "Fold 03 | Epoch 02 | loss 0.5628 | val F1 0.833 | thr 0.649\n",
      "Fold 03 | Epoch 03 | loss 0.2638 | val F1 0.818 | thr 0.566\n",
      "Fold 03 | Epoch 04 | loss 0.1152 | val F1 0.831 | thr 0.831\n",
      "Fold 03 | Epoch 05 | loss 0.0543 | val F1 0.733 | thr 0.441\n",
      "Fold 03 | Epoch 06 | loss 0.0434 | val F1 0.829 | thr 0.548\n",
      "Fold 03 | Epoch 07 | loss 0.0479 | val F1 0.800 | thr 0.249\n",
      "Fold 04 | Epoch 01 | loss 0.8719 | val F1 0.788 | thr 0.553\n",
      "Fold 04 | Epoch 02 | loss 0.5788 | val F1 0.839 | thr 0.733\n",
      "Fold 04 | Epoch 03 | loss 0.2577 | val F1 0.794 | thr 0.523\n",
      "Fold 04 | Epoch 04 | loss 0.1225 | val F1 0.821 | thr 0.833\n",
      "Fold 04 | Epoch 05 | loss 0.1037 | val F1 0.800 | thr 0.547\n",
      "Fold 04 | Epoch 06 | loss 0.0517 | val F1 0.824 | thr 0.505\n",
      "Fold 04 | Epoch 07 | loss 0.0425 | val F1 0.817 | thr 0.303\n",
      "Fold 05 | Epoch 01 | loss 0.8800 | val F1 0.806 | thr 0.531\n",
      "Fold 05 | Epoch 02 | loss 0.6424 | val F1 0.844 | thr 0.580\n",
      "Fold 05 | Epoch 03 | loss 0.3643 | val F1 0.889 | thr 0.558\n",
      "Fold 05 | Epoch 04 | loss 0.1572 | val F1 0.862 | thr 0.294\n",
      "Fold 05 | Epoch 05 | loss 0.0798 | val F1 0.900 | thr 0.831\n",
      "Fold 05 | Epoch 06 | loss 0.0444 | val F1 0.892 | thr 0.440\n",
      "Fold 05 | Epoch 07 | loss 0.0331 | val F1 0.885 | thr 0.291\n",
      "Fold 05 | Epoch 08 | loss 0.0501 | val F1 0.903 | thr 0.893\n",
      "Fold 05 | Epoch 09 | loss 0.0612 | val F1 0.935 | thr 0.326\n",
      "Fold 05 | Epoch 10 | loss 0.0296 | val F1 0.871 | thr 0.399\n",
      "Fold 05 | Epoch 11 | loss 0.0130 | val F1 0.889 | thr 0.182\n",
      "Fold 05 | Epoch 12 | loss 0.0247 | val F1 0.892 | thr 0.177\n",
      "Fold 05 | Epoch 13 | loss 0.0192 | val F1 0.892 | thr 0.176\n",
      "Fold 05 | Epoch 14 | loss 0.0115 | val F1 0.866 | thr 0.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/utkarsh/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:01<00:00, 14.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | Epoch 01 | loss 0.8687 | val F1 0.743 | thr 0.523\n",
      "Fold 01 | Epoch 02 | loss 0.6598 | val F1 0.776 | thr 0.566\n",
      "Fold 01 | Epoch 03 | loss 0.4875 | val F1 0.784 | thr 0.467\n",
      "Fold 01 | Epoch 04 | loss 0.3492 | val F1 0.788 | thr 0.541\n",
      "Fold 01 | Epoch 05 | loss 0.1985 | val F1 0.818 | thr 0.614\n",
      "Fold 01 | Epoch 06 | loss 0.1431 | val F1 0.806 | thr 0.621\n",
      "Fold 01 | Epoch 07 | loss 0.0905 | val F1 0.758 | thr 0.682\n",
      "Fold 01 | Epoch 08 | loss 0.1018 | val F1 0.806 | thr 0.592\n",
      "Fold 01 | Epoch 09 | loss 0.1088 | val F1 0.829 | thr 0.405\n",
      "Fold 01 | Epoch 10 | loss 0.0497 | val F1 0.829 | thr 0.350\n",
      "Fold 01 | Epoch 11 | loss 0.0483 | val F1 0.812 | thr 0.821\n",
      "Fold 01 | Epoch 12 | loss 0.0301 | val F1 0.795 | thr 0.182\n",
      "Fold 01 | Epoch 13 | loss 0.0232 | val F1 0.806 | thr 0.540\n",
      "Fold 01 | Epoch 14 | loss 0.0423 | val F1 0.776 | thr 0.794\n",
      "Fold 02 | Epoch 01 | loss 0.8554 | val F1 0.767 | thr 0.529\n",
      "Fold 02 | Epoch 02 | loss 0.6519 | val F1 0.771 | thr 0.476\n",
      "Fold 02 | Epoch 03 | loss 0.4700 | val F1 0.807 | thr 0.793\n",
      "Fold 02 | Epoch 04 | loss 0.3373 | val F1 0.807 | thr 0.756\n",
      "Fold 02 | Epoch 05 | loss 0.2204 | val F1 0.788 | thr 0.266\n",
      "Fold 02 | Epoch 06 | loss 0.1444 | val F1 0.812 | thr 0.305\n",
      "Fold 02 | Epoch 07 | loss 0.1051 | val F1 0.794 | thr 0.131\n",
      "Fold 02 | Epoch 08 | loss 0.0862 | val F1 0.814 | thr 0.794\n",
      "Fold 02 | Epoch 09 | loss 0.0796 | val F1 0.814 | thr 0.691\n",
      "Fold 02 | Epoch 10 | loss 0.0556 | val F1 0.828 | thr 0.570\n",
      "Fold 02 | Epoch 11 | loss 0.0650 | val F1 0.814 | thr 0.705\n",
      "Fold 02 | Epoch 12 | loss 0.0339 | val F1 0.833 | thr 0.393\n",
      "Fold 02 | Epoch 13 | loss 0.0551 | val F1 0.794 | thr 0.374\n",
      "Fold 02 | Epoch 14 | loss 0.0313 | val F1 0.800 | thr 0.305\n",
      "Fold 02 | Epoch 15 | loss 0.0241 | val F1 0.806 | thr 0.513\n",
      "Fold 02 | Epoch 16 | loss 0.0199 | val F1 0.788 | thr 0.147\n",
      "Fold 02 | Epoch 17 | loss 0.0247 | val F1 0.800 | thr 0.122\n",
      "Fold 03 | Epoch 01 | loss 0.8598 | val F1 0.655 | thr 0.585\n",
      "Fold 03 | Epoch 02 | loss 0.6500 | val F1 0.651 | thr 0.438\n",
      "Fold 03 | Epoch 03 | loss 0.4794 | val F1 0.684 | thr 0.423\n",
      "Fold 03 | Epoch 04 | loss 0.3334 | val F1 0.730 | thr 0.666\n",
      "Fold 03 | Epoch 05 | loss 0.2728 | val F1 0.716 | thr 0.652\n",
      "Fold 03 | Epoch 06 | loss 0.1647 | val F1 0.730 | thr 0.297\n",
      "Fold 03 | Epoch 07 | loss 0.1402 | val F1 0.730 | thr 0.811\n",
      "Fold 03 | Epoch 08 | loss 0.0978 | val F1 0.738 | thr 0.766\n",
      "Fold 03 | Epoch 09 | loss 0.0943 | val F1 0.767 | thr 0.824\n",
      "Fold 03 | Epoch 10 | loss 0.0609 | val F1 0.774 | thr 0.648\n",
      "Fold 03 | Epoch 11 | loss 0.0482 | val F1 0.767 | thr 0.934\n",
      "Fold 03 | Epoch 12 | loss 0.0339 | val F1 0.750 | thr 0.787\n",
      "Fold 03 | Epoch 13 | loss 0.0378 | val F1 0.735 | thr 0.419\n",
      "Fold 03 | Epoch 14 | loss 0.0313 | val F1 0.787 | thr 0.770\n",
      "Fold 03 | Epoch 15 | loss 0.0352 | val F1 0.750 | thr 0.763\n",
      "Fold 03 | Epoch 16 | loss 0.0495 | val F1 0.727 | thr 0.077\n",
      "Fold 03 | Epoch 17 | loss 0.0351 | val F1 0.727 | thr 0.169\n",
      "Fold 03 | Epoch 18 | loss 0.0199 | val F1 0.730 | thr 0.524\n",
      "Fold 03 | Epoch 19 | loss 0.0230 | val F1 0.727 | thr 0.835\n",
      "Fold 04 | Epoch 01 | loss 0.8688 | val F1 0.793 | thr 0.581\n",
      "Fold 04 | Epoch 02 | loss 0.6734 | val F1 0.817 | thr 0.530\n",
      "Fold 04 | Epoch 03 | loss 0.4890 | val F1 0.824 | thr 0.604\n",
      "Fold 04 | Epoch 04 | loss 0.3649 | val F1 0.820 | thr 0.660\n",
      "Fold 04 | Epoch 05 | loss 0.2468 | val F1 0.806 | thr 0.801\n",
      "Fold 04 | Epoch 06 | loss 0.1703 | val F1 0.806 | thr 0.667\n",
      "Fold 04 | Epoch 07 | loss 0.1264 | val F1 0.828 | thr 0.891\n",
      "Fold 04 | Epoch 08 | loss 0.1014 | val F1 0.824 | thr 0.410\n",
      "Fold 04 | Epoch 09 | loss 0.0866 | val F1 0.867 | thr 0.661\n",
      "Fold 04 | Epoch 10 | loss 0.0672 | val F1 0.862 | thr 0.930\n",
      "Fold 04 | Epoch 11 | loss 0.0864 | val F1 0.847 | thr 0.777\n",
      "Fold 04 | Epoch 12 | loss 0.0653 | val F1 0.842 | thr 0.852\n",
      "Fold 04 | Epoch 13 | loss 0.0405 | val F1 0.833 | thr 0.940\n",
      "Fold 04 | Epoch 14 | loss 0.0351 | val F1 0.825 | thr 0.699\n",
      "Fold 05 | Epoch 01 | loss 0.8685 | val F1 0.800 | thr 0.471\n",
      "Fold 05 | Epoch 02 | loss 0.6677 | val F1 0.825 | thr 0.498\n",
      "Fold 05 | Epoch 03 | loss 0.4859 | val F1 0.852 | thr 0.558\n",
      "Fold 05 | Epoch 04 | loss 0.3853 | val F1 0.857 | thr 0.527\n",
      "Fold 05 | Epoch 05 | loss 0.2821 | val F1 0.862 | thr 0.557\n",
      "Fold 05 | Epoch 06 | loss 0.1859 | val F1 0.852 | thr 0.432\n",
      "Fold 05 | Epoch 07 | loss 0.1403 | val F1 0.833 | thr 0.568\n",
      "Fold 05 | Epoch 08 | loss 0.0865 | val F1 0.839 | thr 0.351\n",
      "Fold 05 | Epoch 09 | loss 0.1030 | val F1 0.871 | thr 0.842\n",
      "Fold 05 | Epoch 10 | loss 0.0886 | val F1 0.845 | thr 0.285\n",
      "Fold 05 | Epoch 11 | loss 0.0888 | val F1 0.852 | thr 0.499\n",
      "Fold 05 | Epoch 12 | loss 0.0536 | val F1 0.839 | thr 0.404\n",
      "Fold 05 | Epoch 13 | loss 0.0493 | val F1 0.831 | thr 0.292\n",
      "Fold 05 | Epoch 14 | loss 0.0667 | val F1 0.812 | thr 0.141\n",
      "Fold 01 | Epoch 01 | loss 0.7875 | val F1 0.767 | thr 0.621\n",
      "Fold 01 | Epoch 02 | loss 0.4387 | val F1 0.818 | thr 0.618\n",
      "Fold 01 | Epoch 03 | loss 0.2632 | val F1 0.812 | thr 0.186\n",
      "Fold 01 | Epoch 04 | loss 0.2074 | val F1 0.824 | thr 0.175\n",
      "Fold 01 | Epoch 05 | loss 0.1413 | val F1 0.836 | thr 0.925\n",
      "Fold 01 | Epoch 06 | loss 0.0675 | val F1 0.818 | thr 0.444\n",
      "Fold 01 | Epoch 07 | loss 0.0265 | val F1 0.841 | thr 0.587\n",
      "Fold 01 | Epoch 08 | loss 0.0369 | val F1 0.875 | thr 0.791\n",
      "Fold 01 | Epoch 09 | loss 0.0376 | val F1 0.841 | thr 0.307\n",
      "Fold 01 | Epoch 10 | loss 0.0358 | val F1 0.841 | thr 0.122\n",
      "Fold 01 | Epoch 11 | loss 0.0401 | val F1 0.853 | thr 0.845\n",
      "Fold 01 | Epoch 12 | loss 0.0131 | val F1 0.879 | thr 0.229\n",
      "Fold 01 | Epoch 13 | loss 0.0085 | val F1 0.857 | thr 0.523\n",
      "Fold 01 | Epoch 14 | loss 0.0076 | val F1 0.829 | thr 0.137\n",
      "Fold 01 | Epoch 15 | loss 0.0072 | val F1 0.817 | thr 0.164\n",
      "Fold 01 | Epoch 16 | loss 0.0044 | val F1 0.824 | thr 0.464\n",
      "Fold 01 | Epoch 17 | loss 0.0051 | val F1 0.844 | thr 0.434\n",
      "Fold 02 | Epoch 01 | loss 0.8061 | val F1 0.787 | thr 0.538\n",
      "Fold 02 | Epoch 02 | loss 0.4778 | val F1 0.793 | thr 0.469\n",
      "Fold 02 | Epoch 03 | loss 0.2795 | val F1 0.820 | thr 0.568\n",
      "Fold 02 | Epoch 04 | loss 0.1701 | val F1 0.806 | thr 0.767\n",
      "Fold 02 | Epoch 05 | loss 0.1150 | val F1 0.774 | thr 0.133\n",
      "Fold 02 | Epoch 06 | loss 0.0758 | val F1 0.833 | thr 0.497\n",
      "Fold 02 | Epoch 07 | loss 0.0748 | val F1 0.788 | thr 0.134\n",
      "Fold 02 | Epoch 08 | loss 0.0594 | val F1 0.806 | thr 0.724\n",
      "Fold 02 | Epoch 09 | loss 0.0690 | val F1 0.806 | thr 0.692\n",
      "Fold 02 | Epoch 10 | loss 0.0329 | val F1 0.812 | thr 0.084\n",
      "Fold 02 | Epoch 11 | loss 0.0189 | val F1 0.814 | thr 0.711\n",
      "Fold 03 | Epoch 01 | loss 0.7970 | val F1 0.727 | thr 0.643\n",
      "Fold 03 | Epoch 02 | loss 0.4634 | val F1 0.771 | thr 0.330\n",
      "Fold 03 | Epoch 03 | loss 0.2743 | val F1 0.707 | thr 0.091\n",
      "Fold 03 | Epoch 04 | loss 0.1571 | val F1 0.793 | thr 0.719\n",
      "Fold 03 | Epoch 05 | loss 0.1261 | val F1 0.711 | thr 0.193\n",
      "Fold 03 | Epoch 06 | loss 0.0922 | val F1 0.730 | thr 0.104\n",
      "Fold 03 | Epoch 07 | loss 0.0547 | val F1 0.769 | thr 0.946\n",
      "Fold 03 | Epoch 08 | loss 0.0379 | val F1 0.732 | thr 0.061\n",
      "Fold 03 | Epoch 09 | loss 0.0462 | val F1 0.778 | thr 0.930\n",
      "Fold 04 | Epoch 01 | loss 0.7902 | val F1 0.806 | thr 0.511\n",
      "Fold 04 | Epoch 02 | loss 0.4565 | val F1 0.820 | thr 0.798\n",
      "Fold 04 | Epoch 03 | loss 0.2788 | val F1 0.833 | thr 0.644\n",
      "Fold 04 | Epoch 04 | loss 0.2163 | val F1 0.831 | thr 0.683\n",
      "Fold 04 | Epoch 05 | loss 0.1356 | val F1 0.839 | thr 0.566\n",
      "Fold 04 | Epoch 06 | loss 0.1254 | val F1 0.844 | thr 0.936\n",
      "Fold 04 | Epoch 07 | loss 0.0597 | val F1 0.844 | thr 0.956\n",
      "Fold 04 | Epoch 08 | loss 0.0269 | val F1 0.831 | thr 0.841\n",
      "Fold 04 | Epoch 09 | loss 0.0228 | val F1 0.842 | thr 0.980\n",
      "Fold 04 | Epoch 10 | loss 0.0261 | val F1 0.825 | thr 0.566\n",
      "Fold 04 | Epoch 11 | loss 0.0278 | val F1 0.836 | thr 0.980\n",
      "Fold 05 | Epoch 01 | loss 0.7699 | val F1 0.836 | thr 0.630\n",
      "Fold 05 | Epoch 02 | loss 0.4725 | val F1 0.949 | thr 0.632\n",
      "Fold 05 | Epoch 03 | loss 0.2838 | val F1 0.857 | thr 0.347\n",
      "Fold 05 | Epoch 04 | loss 0.1806 | val F1 0.912 | thr 0.703\n",
      "Fold 05 | Epoch 05 | loss 0.1264 | val F1 0.933 | thr 0.745\n",
      "Fold 05 | Epoch 06 | loss 0.0754 | val F1 0.892 | thr 0.419\n",
      "Fold 05 | Epoch 07 | loss 0.0478 | val F1 0.918 | thr 0.538\n",
      "Fold 01 | Epoch 01 | loss 0.8888 | val F1 0.667 | thr 0.503\n",
      "Fold 01 | Epoch 02 | loss 0.7051 | val F1 0.761 | thr 0.315\n",
      "Fold 01 | Epoch 03 | loss 0.4157 | val F1 0.818 | thr 0.259\n",
      "Fold 01 | Epoch 04 | loss 0.1830 | val F1 0.833 | thr 0.298\n",
      "Fold 01 | Epoch 05 | loss 0.1397 | val F1 0.844 | thr 0.137\n",
      "Fold 01 | Epoch 06 | loss 0.0863 | val F1 0.844 | thr 0.625\n",
      "Fold 01 | Epoch 07 | loss 0.0860 | val F1 0.805 | thr 0.035\n",
      "Fold 01 | Epoch 08 | loss 0.0561 | val F1 0.845 | thr 0.105\n",
      "Fold 01 | Epoch 09 | loss 0.0555 | val F1 0.857 | thr 0.256\n",
      "Fold 01 | Epoch 10 | loss 0.0462 | val F1 0.818 | thr 0.457\n",
      "Fold 01 | Epoch 11 | loss 0.0174 | val F1 0.836 | thr 0.204\n",
      "Fold 01 | Epoch 12 | loss 0.0174 | val F1 0.800 | thr 0.010\n",
      "Fold 01 | Epoch 13 | loss 0.0920 | val F1 0.845 | thr 0.631\n",
      "Fold 01 | Epoch 14 | loss 0.0394 | val F1 0.805 | thr 0.065\n",
      "Fold 02 | Epoch 01 | loss 0.8781 | val F1 0.807 | thr 0.528\n",
      "Fold 02 | Epoch 02 | loss 0.6461 | val F1 0.842 | thr 0.800\n",
      "Fold 02 | Epoch 03 | loss 0.2783 | val F1 0.806 | thr 0.510\n",
      "Fold 02 | Epoch 04 | loss 0.1527 | val F1 0.831 | thr 0.311\n",
      "Fold 02 | Epoch 05 | loss 0.1580 | val F1 0.794 | thr 0.236\n",
      "Fold 02 | Epoch 06 | loss 0.0683 | val F1 0.828 | thr 0.600\n",
      "Fold 02 | Epoch 07 | loss 0.0680 | val F1 0.812 | thr 0.122\n",
      "Fold 03 | Epoch 01 | loss 0.8883 | val F1 0.727 | thr 0.517\n",
      "Fold 03 | Epoch 02 | loss 0.7042 | val F1 0.817 | thr 0.634\n",
      "Fold 03 | Epoch 03 | loss 0.3492 | val F1 0.795 | thr 0.312\n",
      "Fold 03 | Epoch 04 | loss 0.1589 | val F1 0.747 | thr 0.107\n",
      "Fold 03 | Epoch 05 | loss 0.1324 | val F1 0.807 | thr 0.863\n",
      "Fold 03 | Epoch 06 | loss 0.0917 | val F1 0.767 | thr 0.147\n",
      "Fold 03 | Epoch 07 | loss 0.1046 | val F1 0.800 | thr 0.845\n",
      "Fold 04 | Epoch 01 | loss 0.8948 | val F1 0.732 | thr 0.491\n",
      "Fold 04 | Epoch 02 | loss 0.7574 | val F1 0.812 | thr 0.576\n",
      "Fold 04 | Epoch 03 | loss 0.4570 | val F1 0.806 | thr 0.493\n",
      "Fold 04 | Epoch 04 | loss 0.2381 | val F1 0.800 | thr 0.716\n",
      "Fold 04 | Epoch 05 | loss 0.1249 | val F1 0.848 | thr 0.798\n",
      "Fold 04 | Epoch 06 | loss 0.0764 | val F1 0.839 | thr 0.934\n",
      "Fold 04 | Epoch 07 | loss 0.0358 | val F1 0.844 | thr 0.874\n",
      "Fold 04 | Epoch 08 | loss 0.0422 | val F1 0.871 | thr 0.880\n",
      "Fold 04 | Epoch 09 | loss 0.0455 | val F1 0.847 | thr 0.644\n",
      "Fold 04 | Epoch 10 | loss 0.0437 | val F1 0.800 | thr 0.492\n",
      "Fold 04 | Epoch 11 | loss 0.0293 | val F1 0.818 | thr 0.883\n",
      "Fold 04 | Epoch 12 | loss 0.0616 | val F1 0.800 | thr 0.716\n",
      "Fold 04 | Epoch 13 | loss 0.0171 | val F1 0.842 | thr 0.833\n",
      "Fold 05 | Epoch 01 | loss 0.8836 | val F1 0.774 | thr 0.517\n",
      "Fold 05 | Epoch 02 | loss 0.6910 | val F1 0.879 | thr 0.575\n",
      "Fold 05 | Epoch 03 | loss 0.3931 | val F1 0.879 | thr 0.686\n",
      "Fold 05 | Epoch 04 | loss 0.1970 | val F1 0.885 | thr 0.680\n",
      "Fold 05 | Epoch 05 | loss 0.1361 | val F1 0.896 | thr 0.544\n",
      "Fold 05 | Epoch 06 | loss 0.1919 | val F1 0.885 | thr 0.818\n",
      "Fold 05 | Epoch 07 | loss 0.1068 | val F1 0.845 | thr 0.056\n",
      "Fold 05 | Epoch 08 | loss 0.0510 | val F1 0.885 | thr 0.929\n",
      "Fold 05 | Epoch 09 | loss 0.1625 | val F1 0.892 | thr 0.273\n",
      "Fold 05 | Epoch 10 | loss 0.0541 | val F1 0.906 | thr 0.644\n",
      "Fold 05 | Epoch 11 | loss 0.0445 | val F1 0.906 | thr 0.466\n",
      "Fold 05 | Epoch 12 | loss 0.0099 | val F1 0.912 | thr 0.988\n",
      "Fold 05 | Epoch 13 | loss 0.0376 | val F1 0.871 | thr 0.512\n",
      "Fold 05 | Epoch 14 | loss 0.0068 | val F1 0.881 | thr 0.442\n",
      "Fold 05 | Epoch 15 | loss 0.0218 | val F1 0.903 | thr 0.333\n",
      "Fold 05 | Epoch 16 | loss 0.0287 | val F1 0.921 | thr 0.678\n",
      "Fold 05 | Epoch 17 | loss 0.0102 | val F1 0.935 | thr 0.606\n",
      "Fold 05 | Epoch 18 | loss 0.0078 | val F1 0.889 | thr 0.675\n",
      "Fold 05 | Epoch 19 | loss 0.0064 | val F1 0.879 | thr 0.045\n",
      "Fold 05 | Epoch 20 | loss 0.0108 | val F1 0.923 | thr 0.142\n",
      "Fold 05 | Epoch 21 | loss 0.0061 | val F1 0.929 | thr 0.974\n",
      "Fold 05 | Epoch 22 | loss 0.0030 | val F1 0.933 | thr 0.389\n",
      "Fold 01 | Epoch 01 | loss 0.8719 | val F1 0.733 | thr 0.554\n",
      "Fold 01 | Epoch 02 | loss 0.7281 | val F1 0.800 | thr 0.660\n",
      "Fold 01 | Epoch 03 | loss 0.4966 | val F1 0.778 | thr 0.612\n",
      "Fold 01 | Epoch 04 | loss 0.3160 | val F1 0.783 | thr 0.567\n",
      "Fold 01 | Epoch 05 | loss 0.2136 | val F1 0.831 | thr 0.788\n",
      "Fold 01 | Epoch 06 | loss 0.1328 | val F1 0.825 | thr 0.852\n",
      "Fold 01 | Epoch 07 | loss 0.0798 | val F1 0.831 | thr 0.680\n",
      "Fold 01 | Epoch 08 | loss 0.1003 | val F1 0.831 | thr 0.583\n",
      "Fold 01 | Epoch 09 | loss 0.0576 | val F1 0.831 | thr 0.263\n",
      "Fold 01 | Epoch 10 | loss 0.0482 | val F1 0.812 | thr 0.059\n",
      "Fold 02 | Epoch 01 | loss 0.8824 | val F1 0.765 | thr 0.518\n",
      "Fold 02 | Epoch 02 | loss 0.7424 | val F1 0.818 | thr 0.502\n",
      "Fold 02 | Epoch 03 | loss 0.5359 | val F1 0.807 | thr 0.663\n",
      "Fold 02 | Epoch 04 | loss 0.4033 | val F1 0.862 | thr 0.544\n",
      "Fold 02 | Epoch 05 | loss 0.2469 | val F1 0.844 | thr 0.599\n",
      "Fold 02 | Epoch 06 | loss 0.1868 | val F1 0.833 | thr 0.721\n",
      "Fold 02 | Epoch 07 | loss 0.1066 | val F1 0.812 | thr 0.378\n",
      "Fold 02 | Epoch 08 | loss 0.0498 | val F1 0.862 | thr 0.899\n",
      "Fold 02 | Epoch 09 | loss 0.0720 | val F1 0.871 | thr 0.699\n",
      "Fold 02 | Epoch 10 | loss 0.0353 | val F1 0.842 | thr 0.667\n",
      "Fold 02 | Epoch 11 | loss 0.0661 | val F1 0.817 | thr 0.152\n",
      "Fold 02 | Epoch 12 | loss 0.0461 | val F1 0.818 | thr 0.405\n",
      "Fold 02 | Epoch 13 | loss 0.0394 | val F1 0.862 | thr 0.941\n",
      "Fold 02 | Epoch 14 | loss 0.0442 | val F1 0.818 | thr 0.299\n",
      "Fold 03 | Epoch 01 | loss 0.8925 | val F1 0.717 | thr 0.502\n",
      "Fold 03 | Epoch 02 | loss 0.7558 | val F1 0.647 | thr 0.472\n",
      "Fold 03 | Epoch 03 | loss 0.5846 | val F1 0.685 | thr 0.478\n",
      "Fold 03 | Epoch 04 | loss 0.3988 | val F1 0.700 | thr 0.621\n",
      "Fold 03 | Epoch 05 | loss 0.3025 | val F1 0.727 | thr 0.378\n",
      "Fold 03 | Epoch 06 | loss 0.1882 | val F1 0.730 | thr 0.591\n",
      "Fold 03 | Epoch 07 | loss 0.1327 | val F1 0.754 | thr 0.877\n",
      "Fold 03 | Epoch 08 | loss 0.0749 | val F1 0.746 | thr 0.798\n",
      "Fold 03 | Epoch 09 | loss 0.0808 | val F1 0.759 | thr 0.817\n",
      "Fold 03 | Epoch 10 | loss 0.1050 | val F1 0.750 | thr 0.949\n",
      "Fold 03 | Epoch 11 | loss 0.0392 | val F1 0.735 | thr 0.317\n",
      "Fold 03 | Epoch 12 | loss 0.0487 | val F1 0.735 | thr 0.378\n",
      "Fold 03 | Epoch 13 | loss 0.0621 | val F1 0.762 | thr 0.870\n",
      "Fold 03 | Epoch 14 | loss 0.0568 | val F1 0.774 | thr 0.892\n",
      "Fold 03 | Epoch 15 | loss 0.0395 | val F1 0.758 | thr 0.445\n",
      "Fold 03 | Epoch 16 | loss 0.0570 | val F1 0.760 | thr 0.966\n",
      "Fold 03 | Epoch 17 | loss 0.0380 | val F1 0.762 | thr 0.651\n",
      "Fold 03 | Epoch 18 | loss 0.0123 | val F1 0.762 | thr 0.732\n",
      "Fold 03 | Epoch 19 | loss 0.0349 | val F1 0.762 | thr 0.400\n",
      "Fold 04 | Epoch 01 | loss 0.8767 | val F1 0.735 | thr 0.572\n",
      "Fold 04 | Epoch 02 | loss 0.7274 | val F1 0.778 | thr 0.583\n",
      "Fold 04 | Epoch 03 | loss 0.5163 | val F1 0.789 | thr 0.570\n",
      "Fold 04 | Epoch 04 | loss 0.2844 | val F1 0.794 | thr 0.778\n",
      "Fold 04 | Epoch 05 | loss 0.1893 | val F1 0.787 | thr 0.817\n",
      "Fold 04 | Epoch 06 | loss 0.1336 | val F1 0.806 | thr 0.577\n",
      "Fold 04 | Epoch 07 | loss 0.1085 | val F1 0.818 | thr 0.515\n",
      "Fold 04 | Epoch 08 | loss 0.1130 | val F1 0.820 | thr 0.931\n",
      "Fold 04 | Epoch 09 | loss 0.1288 | val F1 0.839 | thr 0.859\n",
      "Fold 04 | Epoch 10 | loss 0.0752 | val F1 0.833 | thr 0.622\n",
      "Fold 04 | Epoch 11 | loss 0.0738 | val F1 0.825 | thr 0.887\n",
      "Fold 04 | Epoch 12 | loss 0.0356 | val F1 0.812 | thr 0.893\n",
      "Fold 04 | Epoch 13 | loss 0.0648 | val F1 0.820 | thr 0.733\n",
      "Fold 04 | Epoch 14 | loss 0.0512 | val F1 0.833 | thr 0.915\n",
      "Fold 05 | Epoch 01 | loss 0.8755 | val F1 0.857 | thr 0.514\n",
      "Fold 05 | Epoch 02 | loss 0.7738 | val F1 0.862 | thr 0.482\n",
      "Fold 05 | Epoch 03 | loss 0.5968 | val F1 0.867 | thr 0.638\n",
      "Fold 05 | Epoch 04 | loss 0.4106 | val F1 0.847 | thr 0.409\n",
      "Fold 05 | Epoch 05 | loss 0.2432 | val F1 0.871 | thr 0.428\n",
      "Fold 05 | Epoch 06 | loss 0.1489 | val F1 0.862 | thr 0.889\n",
      "Fold 05 | Epoch 07 | loss 0.1099 | val F1 0.844 | thr 0.352\n",
      "Fold 05 | Epoch 08 | loss 0.1040 | val F1 0.836 | thr 0.327\n",
      "Fold 05 | Epoch 09 | loss 0.0853 | val F1 0.857 | thr 0.906\n",
      "Fold 05 | Epoch 10 | loss 0.0600 | val F1 0.857 | thr 0.537\n"
     ]
    }
   ],
   "source": [
    "for v in [\"resnet_overlay\", \"fusion\"]:\n",
    "    for b in [\"resnet18\", \"resnet50\", \"efficientnet_b0\"]:\n",
    "        print(\"\\n--------------------------------\\n\")\n",
    "        print(f\"Running {v} with {b}\")\n",
    "        run_experiment(df_with_masks, variant=v, backbone=b, out_dir=f\"mask_quality/{v}_{b}\")\n",
    "        print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dad953f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Comparison + Majority-Vote Ensemble (for Mask Usability Classifier v2)\n",
    "-----------------------------------------------------------------------------\n",
    "This helper script does two things:\n",
    "\n",
    "1) Collects all model_cards from /mnt/data/mask_quality/** and prints a table\n",
    "   comparing variants/backbones by mean F1 (and other metrics). Saves CSV.\n",
    "\n",
    "2) Loads all trained checkpoints (best_overall.pt) and runs prediction on a\n",
    "   given DataFrame (e.g., tree_data_merged.csv). It produces 6 predictions per\n",
    "   row (for the 2 variants × 3 backbones), then computes a majority-vote\n",
    "   ensemble and reports metrics. Saves predictions CSV.\n",
    "\n",
    "Notes:\n",
    "- Expects the v2 module available as `mask_usability_classifier` with\n",
    "  load_for_inference(...) and predict_row(...).\n",
    "- If your CSV stores `mask_data` as a string, we attempt to parse it via JSON\n",
    "  or ast.literal_eval. If mask is still missing, Fusion models will skip that\n",
    "  row (vote uses only available model predictions for that row).\n",
    "- Ensemble majority uses a conservative tie-break: ratio > 0.5 → positive; else 0.\n",
    "\n",
    "Usage example (in a notebook):\n",
    "\n",
    "    import pandas as pd\n",
    "    from model_compare_and_ensemble import (\n",
    "        collect_model_cards, compare_models_table,\n",
    "        load_all_infers, predict_all_models,\n",
    "        compute_metrics, evaluate_ensemble\n",
    "    )\n",
    "\n",
    "    cards_df = compare_models_table(root=\"/mnt/data/mask_quality\")\n",
    "    display(cards_df)\n",
    "\n",
    "    df = pd.read_csv(\"/mnt/data/tree_data_merged.csv\")\n",
    "    preds_df = predict_all_models(df, root=\"/mnt/data/mask_quality\",\n",
    "                                  image_key=\"image_path\", mask_key=\"mask_data\")\n",
    "\n",
    "    # Metrics per individual model and the ensemble\n",
    "    for col in [c for c in preds_df.columns if c.startswith(\"pred__\")] + [\"ensemble_pred\"]:\n",
    "        m = compute_metrics(preds_df[\"correct\"], preds_df[col])\n",
    "        print(col, m)\n",
    "\n",
    "    # Quick ensemble report\n",
    "    print(evaluate_ensemble(preds_df))\n",
    "\n",
    "Outputs saved to:\n",
    "- /mnt/data/mask_quality/model_comparison.csv\n",
    "- /mnt/data/mask_quality/ensemble_predictions.csv\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Collect & compare model cards\n",
    "# -----------------------------\n",
    "\n",
    "def _find_all_cards(root: str) -> List[Dict[str, Any]]:\n",
    "    cards: List[Dict[str, Any]] = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        if \"model_card.json\" in filenames:\n",
    "            p = os.path.join(dirpath, \"model_card.json\")\n",
    "            try:\n",
    "                with open(p, \"r\") as f:\n",
    "                    card = json.load(f)\n",
    "                card[\"_card_path\"] = p\n",
    "                cards.append(card)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return cards\n",
    "\n",
    "\n",
    "def collect_model_cards(root: str = \"/mnt/data/mask_quality\") -> pd.DataFrame:\n",
    "    cards = _find_all_cards(root)\n",
    "    rows = []\n",
    "    for c in cards:\n",
    "        s = c.get(\"summary\", {})\n",
    "        rows.append({\n",
    "            \"dir\": os.path.dirname(c.get(\"_card_path\", \"\")),\n",
    "            \"variant\": c.get(\"variant\"),\n",
    "            \"backbone\": c.get(\"backbone\", \"resnet18\"),\n",
    "            \"f1_mean\": s.get(\"f1_mean\"), \"f1_std\": s.get(\"f1_std\"),\n",
    "            \"acc_mean\": s.get(\"acc_mean\"), \"prec_mean\": s.get(\"prec_mean\"),\n",
    "            \"rec_mean\": s.get(\"rec_mean\"), \"auc_mean\": s.get(\"auc_mean\"),\n",
    "            \"best_ckpt\": c.get(\"best_overall\", {}).get(\"path\"),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_models_table(root: str = \"/mnt/data/mask_quality\") -> pd.DataFrame:\n",
    "    df = collect_model_cards(root)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values([\"f1_mean\", \"acc_mean\"], ascending=[False, False])\n",
    "        out = os.path.join(root, \"model_comparison.csv\")\n",
    "        df.to_csv(out, index=False)\n",
    "        print(f\"Saved: {out}\")\n",
    "    else:\n",
    "        print(\"No model_card.json files found under\", root)\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load all models & batch predict\n",
    "# -----------------------------\n",
    "\n",
    "def _list_best_checkpoints(root: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Return list of (name, best_ckpt_path). Name is derived from directory name.\n",
    "    Looks for 'best_overall.pt' in each leaf directory.\n",
    "    \"\"\"\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        if \"best_overall.pt\" in filenames:\n",
    "            name = os.path.basename(dirpath)  # e.g., 'resnet_overlay_resnet50' or 'fusion_efficientnet_b0'\n",
    "            pairs.append((name, os.path.join(dirpath, \"best_overall.pt\")))\n",
    "    return sorted(pairs)\n",
    "\n",
    "\n",
    "def load_all_infers(root: str = \"/mnt/data/mask_quality\") -> Dict[str, InferenceBundle]:\n",
    "    pairs = _list_best_checkpoints(root)\n",
    "    infers: Dict[str, InferenceBundle] = {}\n",
    "    for name, ckpt in pairs:\n",
    "        try:\n",
    "            infers[name] = load_for_inference(ckpt)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {name} @ {ckpt}: {e}\")\n",
    "    if not infers:\n",
    "        raise RuntimeError(f\"No best_overall.pt found under {root}\")\n",
    "    return infers\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Robust mask parsing for CSVs\n",
    "# -----------------------------\n",
    "\n",
    "def _parse_mask_cell(cell: Any) -> Any:\n",
    "    \"\"\"Try to recover a mask object from CSV cell. Returns a dict/np array or None.\n",
    "    Accepts: dict already, JSON string, python-literal string, or None.\n",
    "    \"\"\"\n",
    "    if isinstance(cell, (dict, np.ndarray)):\n",
    "        return cell\n",
    "    if cell is None or (isinstance(cell, float) and np.isnan(cell)):\n",
    "        return None\n",
    "    if isinstance(cell, str):\n",
    "        s = cell.strip()\n",
    "        # Try JSON first\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Try python literal (lists, tuples, dicts, arrays as lists)\n",
    "        try:\n",
    "            return literal_eval(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Predict all models and build ensemble\n",
    "# -----------------------------\n",
    "\n",
    "def predict_all_models(df: pd.DataFrame,\n",
    "                       root: str = \"/mnt/data/mask_quality\",\n",
    "                       image_key: str = \"image_path\",\n",
    "                       mask_key: str = \"mask_data\",\n",
    "                       require_mask_for_fusion: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Run every saved model on every row and return a predictions DataFrame.\n",
    "\n",
    "    Columns added:\n",
    "      - pred__{model_name}  ∈ {0,1}\n",
    "      - prob__{model_name}  ∈ [0,1]\n",
    "      - ensemble_pred, ensemble_prob (vote ratio)\n",
    "\n",
    "    If a Fusion model needs a mask but none can be parsed for a row:\n",
    "      - if require_mask_for_fusion=False (default), we skip that model for that row\n",
    "        (its pred/prob are NaN), and the ensemble uses the remaining votes.\n",
    "      - if True, we drop rows lacking masks before prediction.\n",
    "    \"\"\"\n",
    "    infers = load_all_infers(root)\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure mask column is parsed (if present)\n",
    "    if mask_key in df.columns:\n",
    "        df[mask_key] = df[mask_key].apply(_parse_mask_cell)\n",
    "\n",
    "    # Optional: drop rows without mask if we must support Fusion\n",
    "    if require_mask_for_fusion:\n",
    "        # Keep only rows with a parseable mask\n",
    "        has_mask = df[mask_key].apply(lambda x: x is not None)\n",
    "        df = df[has_mask].reset_index(drop=True)\n",
    "\n",
    "    # Prepare output columns\n",
    "    for name in infers.keys():\n",
    "        df[f\"pred__{name}\"] = np.nan\n",
    "        df[f\"prob__{name}\"] = np.nan\n",
    "\n",
    "    # Iterate rows\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        for name, infer in infers.items():\n",
    "            try:\n",
    "                # If fusion and mask missing, skip this model for this row\n",
    "                if infer.variant == 'fusion' and mask_key in df.columns and row[mask_key] is None:\n",
    "                    continue\n",
    "                yb, pr = predict_row(row, infer, image_key=image_key, mask_key=mask_key)\n",
    "                df.at[i, f\"pred__{name}\"] = int(yb)\n",
    "                df.at[i, f\"prob__{name}\"] = float(pr)\n",
    "            except Exception as e:\n",
    "                # Keep NaNs on failure\n",
    "                pass\n",
    "\n",
    "    # Ensemble: majority over available predictions per row\n",
    "    pred_cols = [c for c in df.columns if c.startswith(\"pred__\")]\n",
    "    pred_mat = df[pred_cols].to_numpy(dtype=float)  # NaNs possible\n",
    "    votes = np.nansum(pred_mat, axis=1)\n",
    "    counts = np.sum(~np.isnan(pred_mat), axis=1)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        ratio = votes / counts\n",
    "    # Conservative tie-break: strictly greater than 0.5\n",
    "    ens_pred = (ratio > 0.5).astype(float)\n",
    "    # Where no models predicted (counts==0), set to NaN\n",
    "    ens_pred[counts == 0] = np.nan\n",
    "\n",
    "    df[\"ensemble_prob\"] = ratio\n",
    "    df[\"ensemble_pred\"] = ens_pred\n",
    "\n",
    "    # Save predictions\n",
    "    out_csv = os.path.join(root, \"ensemble_predictions.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved: {out_csv}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Metrics\n",
    "# -----------------------------\n",
    "\n",
    "def _label_to_int(arr: pd.Series | np.ndarray) -> np.ndarray:\n",
    "    return np.array([_to_bool_label(x) for x in arr], dtype=int)\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: pd.Series | np.ndarray, y_pred: pd.Series | np.ndarray) -> Dict[str, Any]:\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "    y = _label_to_int(y_true)\n",
    "    yp = pd.Series(y_pred).astype(float).to_numpy()\n",
    "    mask = ~np.isnan(yp)\n",
    "    y = y[mask]\n",
    "    yp = yp[mask].astype(int)\n",
    "    if len(y) == 0:\n",
    "        return {\"n\": 0, \"acc\": np.nan, \"prec\": np.nan, \"rec\": np.nan, \"f1\": np.nan, \"cm\": [[0,0],[0,0]]}\n",
    "    acc = accuracy_score(y, yp)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y, yp, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y, yp).tolist()\n",
    "    return {\"n\": int(len(y)), \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"cm\": cm}\n",
    "\n",
    "\n",
    "def evaluate_ensemble(df_preds: pd.DataFrame, label_col: str = \"correct\") -> Dict[str, Any]:\n",
    "    # Individual model metrics\n",
    "    per_model = {}\n",
    "    for c in [c for c in df_preds.columns if c.startswith(\"pred__\")]:\n",
    "        per_model[c] = compute_metrics(df_preds[label_col], df_preds[c])\n",
    "\n",
    "    # Ensemble metrics\n",
    "    ens = compute_metrics(df_preds[label_col], df_preds[\"ensemble_pred\"])\n",
    "\n",
    "    # Pack\n",
    "    return {\n",
    "        \"ensemble\": ens,\n",
    "        \"per_model\": per_model,\n",
    "        \"n_rows\": int(len(df_preds)),\n",
    "        \"n_rows_with_any_vote\": int(np.sum(~np.isnan(df_preds[\"ensemble_pred\"]))),\n",
    "        \"vote_counts_summary\": df_preds[[c for c in df_preds.columns if c.startswith(\"pred__\")]]\n",
    "            .notna().sum(axis=1).describe().to_dict(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0616602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: mask_quality/model_comparison.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>variant</th>\n",
       "      <th>backbone</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>prec_mean</th>\n",
       "      <th>rec_mean</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>best_ckpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mask_quality/fusion_resnet50</td>\n",
       "      <td>fusion</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>0.864520</td>\n",
       "      <td>0.039754</td>\n",
       "      <td>0.902503</td>\n",
       "      <td>0.837991</td>\n",
       "      <td>0.900860</td>\n",
       "      <td>0.936630</td>\n",
       "      <td>mask_quality/fusion_resnet50/fusion_resnet50_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mask_quality/resnet_overlay_resnet50</td>\n",
       "      <td>resnet_overlay</td>\n",
       "      <td>resnet50</td>\n",
       "      <td>0.860515</td>\n",
       "      <td>0.038029</td>\n",
       "      <td>0.897983</td>\n",
       "      <td>0.821142</td>\n",
       "      <td>0.913763</td>\n",
       "      <td>0.920645</td>\n",
       "      <td>mask_quality/resnet_overlay_resnet50/resnet_ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mask_quality/fusion_resnet18</td>\n",
       "      <td>fusion</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>0.859625</td>\n",
       "      <td>0.052442</td>\n",
       "      <td>0.902477</td>\n",
       "      <td>0.848594</td>\n",
       "      <td>0.873763</td>\n",
       "      <td>0.928050</td>\n",
       "      <td>mask_quality/fusion_resnet18/fusion_resnet18_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mask_quality/resnet_overlay_resnet18</td>\n",
       "      <td>resnet_overlay</td>\n",
       "      <td>resnet18</td>\n",
       "      <td>0.841338</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>0.886645</td>\n",
       "      <td>0.823654</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.926663</td>\n",
       "      <td>mask_quality/resnet_overlay_resnet18/resnet_ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mask_quality/resnet_overlay_efficientnet_b0</td>\n",
       "      <td>resnet_overlay</td>\n",
       "      <td>efficientnet_b0</td>\n",
       "      <td>0.837285</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.884397</td>\n",
       "      <td>0.812307</td>\n",
       "      <td>0.867097</td>\n",
       "      <td>0.913359</td>\n",
       "      <td>mask_quality/resnet_overlay_efficientnet_b0/re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mask_quality/fusion_efficientnet_b0</td>\n",
       "      <td>fusion</td>\n",
       "      <td>efficientnet_b0</td>\n",
       "      <td>0.837122</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.884372</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.914839</td>\n",
       "      <td>mask_quality/fusion_efficientnet_b0/fusion_eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           dir         variant  \\\n",
       "2                 mask_quality/fusion_resnet50          fusion   \n",
       "5         mask_quality/resnet_overlay_resnet50  resnet_overlay   \n",
       "4                 mask_quality/fusion_resnet18          fusion   \n",
       "0         mask_quality/resnet_overlay_resnet18  resnet_overlay   \n",
       "3  mask_quality/resnet_overlay_efficientnet_b0  resnet_overlay   \n",
       "1          mask_quality/fusion_efficientnet_b0          fusion   \n",
       "\n",
       "          backbone   f1_mean    f1_std  acc_mean  prec_mean  rec_mean  \\\n",
       "2         resnet50  0.864520  0.039754  0.902503   0.837991  0.900860   \n",
       "5         resnet50  0.860515  0.038029  0.897983   0.821142  0.913763   \n",
       "4         resnet18  0.859625  0.052442  0.902477   0.848594  0.873763   \n",
       "0         resnet18  0.841338  0.026632  0.886645   0.823654  0.867527   \n",
       "3  efficientnet_b0  0.837285  0.030429  0.884397   0.812307  0.867097   \n",
       "1  efficientnet_b0  0.837122  0.035479  0.884372   0.808824  0.867527   \n",
       "\n",
       "   auc_mean                                          best_ckpt  \n",
       "2  0.936630  mask_quality/fusion_resnet50/fusion_resnet50_f...  \n",
       "5  0.920645  mask_quality/resnet_overlay_resnet50/resnet_ov...  \n",
       "4  0.928050  mask_quality/fusion_resnet18/fusion_resnet18_f...  \n",
       "0  0.926663  mask_quality/resnet_overlay_resnet18/resnet_ov...  \n",
       "3  0.913359  mask_quality/resnet_overlay_efficientnet_b0/re...  \n",
       "1  0.914839  mask_quality/fusion_efficientnet_b0/fusion_eff...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2133266/181111717.py:587: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: mask_quality/ensemble_predictions.csv\n",
      "pred__fusion_efficientnet_b0 {'n': 0, 'acc': nan, 'prec': nan, 'rec': nan, 'f1': nan, 'cm': [[0, 0], [0, 0]]}\n",
      "pred__fusion_resnet18 {'n': 0, 'acc': nan, 'prec': nan, 'rec': nan, 'f1': nan, 'cm': [[0, 0], [0, 0]]}\n",
      "pred__fusion_resnet50 {'n': 0, 'acc': nan, 'prec': nan, 'rec': nan, 'f1': nan, 'cm': [[0, 0], [0, 0]]}\n",
      "pred__resnet_overlay_efficientnet_b0 {'n': 441, 'acc': 0.981859410430839, 'prec': np.float64(0.9673202614379085), 'rec': np.float64(0.9801324503311258), 'f1': np.float64(0.9736842105263158), 'cm': [[285, 5], [3, 148]]}\n",
      "pred__resnet_overlay_resnet18 {'n': 441, 'acc': 0.981859410430839, 'prec': np.float64(0.9798657718120806), 'rec': np.float64(0.9668874172185431), 'f1': np.float64(0.9733333333333334), 'cm': [[287, 3], [5, 146]]}\n",
      "pred__resnet_overlay_resnet50 {'n': 441, 'acc': 0.9886621315192744, 'prec': np.float64(0.9802631578947368), 'rec': np.float64(0.9867549668874173), 'f1': np.float64(0.9834983498349835), 'cm': [[287, 3], [2, 149]]}\n",
      "ensemble_pred {'n': 441, 'acc': 0.9863945578231292, 'prec': np.float64(0.9801324503311258), 'rec': np.float64(0.9801324503311258), 'f1': np.float64(0.9801324503311258), 'cm': [[287, 3], [3, 148]]}\n",
      "Ensemble summary: {'ensemble': {'n': 441, 'acc': 0.9863945578231292, 'prec': np.float64(0.9801324503311258), 'rec': np.float64(0.9801324503311258), 'f1': np.float64(0.9801324503311258), 'cm': [[287, 3], [3, 148]]}, 'per_model': {'pred__fusion_efficientnet_b0': {'n': 0, 'acc': nan, 'prec': nan, 'rec': nan, 'f1': nan, 'cm': [[0, 0], [0, 0]]}, 'pred__fusion_resnet18': {'n': 0, 'acc': nan, 'prec': nan, 'rec': nan, 'f1': nan, 'cm': [[0, 0], [0, 0]]}, 'pred__fusion_resnet50': {'n': 0, 'acc': nan, 'prec': nan, 'rec': nan, 'f1': nan, 'cm': [[0, 0], [0, 0]]}, 'pred__resnet_overlay_efficientnet_b0': {'n': 441, 'acc': 0.981859410430839, 'prec': np.float64(0.9673202614379085), 'rec': np.float64(0.9801324503311258), 'f1': np.float64(0.9736842105263158), 'cm': [[285, 5], [3, 148]]}, 'pred__resnet_overlay_resnet18': {'n': 441, 'acc': 0.981859410430839, 'prec': np.float64(0.9798657718120806), 'rec': np.float64(0.9668874172185431), 'f1': np.float64(0.9733333333333334), 'cm': [[287, 3], [5, 146]]}, 'pred__resnet_overlay_resnet50': {'n': 441, 'acc': 0.9886621315192744, 'prec': np.float64(0.9802631578947368), 'rec': np.float64(0.9867549668874173), 'f1': np.float64(0.9834983498349835), 'cm': [[287, 3], [2, 149]]}}, 'n_rows': 441, 'n_rows_with_any_vote': 441, 'vote_counts_summary': {'count': 441.0, 'mean': 3.0, 'std': 0.0, 'min': 3.0, '25%': 3.0, '50%': 3.0, '75%': 3.0, 'max': 3.0}}\n"
     ]
    }
   ],
   "source": [
    "# 1) Compare all trained models (reads every model_card.json)\n",
    "cards_df = compare_models_table(root=\"mask_quality\")\n",
    "display(cards_df)\n",
    "\n",
    "# 2) Predict with all six models on your CSV\n",
    "df = pd.read_csv(\"tree_data_merged.csv\")\n",
    "preds_df = predict_all_models(\n",
    "    df, \n",
    "    root=\"mask_quality\",\n",
    "    image_key=\"image_path\",\n",
    "    mask_key=\"mask_data\",          # if masks are strings, this function will try to parse them\n",
    "    require_mask_for_fusion=False  # keep False to allow partial votes when masks are missing\n",
    ")\n",
    "\n",
    "# 3) Metrics per model and for the majority-vote ensemble\n",
    "for col in [c for c in preds_df.columns if c.startswith(\"pred__\")] + [\"ensemble_pred\"]:\n",
    "    m = compute_metrics(preds_df[\"correct\"], preds_df[col])\n",
    "    print(col, m)\n",
    "\n",
    "print(\"Ensemble summary:\", evaluate_ensemble(preds_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047aa42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
